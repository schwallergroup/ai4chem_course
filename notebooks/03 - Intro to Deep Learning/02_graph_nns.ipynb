{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdc997b6-501f-464d-a388-bf3c9540400e",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/schwallergroup/ai4chem_course/blob/main/notebooks/03%20-%20Intro%20to%20Deep%20Learning/02_graph_nns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2361f4a6-2cc4-4347-aa21-f557e606bf6f",
   "metadata": {},
   "source": [
    "# Week 3 tutorial 2 - AI 4 Chemistry\n",
    "\n",
    "## Table of content\n",
    "\n",
    "0. Relevant packages\n",
    "1. Inductive biases\n",
    "2. Graph neural network in chemistry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179733c6",
   "metadata": {},
   "source": [
    "# 0. Relevant packages\n",
    "\n",
    "### Pytorch Geometric (PyG)\n",
    "PyG is a library built upon PyTorch to easily write and train Graph Neural Networks (GNNs) for a wide range of applications related to structured data. You can also browse its [documentation](https://pytorch-geometric.readthedocs.io/en/latest/) for additional details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519f98ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all libraries\n",
    "# CoLab has already preinstalled Pytorch for you\n",
    "! pip install pytorch-lightning wandb rdkit ogb deepchem\n",
    "# install PyG\n",
    "import torch\n",
    "VERSION = torch.__version__\n",
    "! pip install pyg_lib torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-{VERSION}.html\n",
    "! pip install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19b18b9",
   "metadata": {},
   "source": [
    "Set a random seed to ensure repeatability of experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55c1555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Random Seeds and Reproducibility\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b8266-ec53-44a3-8051-99f40a358fb4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "One of the promises of deep learning algorithms is that they can learn to `automatically extract features from the raw data`.\\\n",
    "**However, so far we have used the same featurization methods as we used for the more basic models.**\n",
    "\n",
    "> Can our models directly take a molecule as input?\n",
    "\n",
    "# 1. Inductive biases\n",
    "\n",
    "**Inductive biases** are assumptions we make about the data, that help our models extract signal from it. These assumptions are encoded in the model's architecture.\n",
    "\n",
    "For instance, when we (humans) look at images, we think differently than when we read a book, or than when we analyze a molecule. **Processing all these different types of data requires different ways of interpretation, and thus different assumptions about the data**.\n",
    "\n",
    "When building models, we attempt to encode these inductive biases in our model's architecture so they know how to read and process the data.\n",
    "\n",
    "A natural way of representing molecules is as graphs. A graph is a collection of nodes (atoms) and edges (bonds). \n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"img/Chloroquine-2D-molecular-graph.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "In the end, this is what we assume from the data:\n",
    "\n",
    "> Molecules are formed by atoms connected by bonds, and each atom is influenced mostly by its closest neighbors.\\\n",
    "> Molecular properties are determined solely by the molecular graph.\n",
    "\n",
    "This is what **we assume** and thus what **we tell our model**. The specific details of how to calculate the solubility of a molecule (or any other property), that's exactly what the model will try to learn from the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95947e57",
   "metadata": {},
   "source": [
    "# 2. Graph neural network in chemistry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de6865",
   "metadata": {},
   "source": [
    "## 2.1 Graph representation\n",
    "\n",
    "In [graph theory](https://en.wikipedia.org/wiki/Graph_theory), a graph $G=(V,E)$ is defined by a set of **nodes** (also called **vertices**) $V$ and a set of **edges** (also called **links**) $E$ between these vertices. More specifically:\n",
    "\n",
    "- $V = \\{ v_1, \\: ..., \\: v_n \\}$, a set of nodes;\n",
    "- $E \\subseteq \\{ (i,j) \\: | \\: i,j \\in V,  \\: i \\neq j \\}$, a set of edges representing connections between nodes.\n",
    "\n",
    "If the edges of a graph have directions, the graph is called a directed graph, otherwise it is called an undirected graph.\n",
    "\n",
    "<center width=\"100%\" style=\"padding:10px\"><img src=\"img/graphs.png\" width=\"250px\"></center>\n",
    "\n",
    "In many cases we also have attribute or feature information associated with a graph:\n",
    "- node features: $\\mathbf{X} = [..., \\: x_i, \\: ...]^T \\in \\mathbb{R}^{|V| \\times m}$, and $x_i \\in \\mathbb{R}^m$ denotes the feature of node $i$;\n",
    "- edge features: $\\mathbf{L} = [..., \\: l_{i,j}, \\: ...]^T \\in \\mathbb{R}^{|E| \\times r}$, and $l_{i,j} \\in \\mathbb{R}^r$ denotes the feature of the edge between node $i$ and node $j$;\n",
    "- graph features: $\\mathbf{G} = (..., \\: g_i, \\: ...) \\in \\mathbb{R}^s$, and $g_i$ is the feature (or label) $i$ of the graph, which is usually the prediction target.\n",
    "\n",
    "For instance, let's look at the following undirected graph with node features:\n",
    "\n",
    "<center width=\"100%\" style=\"padding:10px\"><img src=\"img/graph_example.svg\" width=\"250px\"></center>\n",
    "\n",
    "This graph has 4 nodes and 4 edges. The nodes are $V=\\{1,2,3,4\\}$, and edges $E=\\{(1,2), (2,3), (2,4), (3,4)\\}$. Note that for simplicity, we don't add mirrored pairs like $(2,1)$. And we have the following node features:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "    0 & 1 & 2\\\\\n",
    "    1 & 0 & 1\\\\\n",
    "    1 & 1 & 0\\\\\n",
    "    3 & 1 & 4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The **adjacency matrix** $A$ is a square matrix whose elements indicate whether pairs of nodes are adjacent, i.e. connected, or not. In the simplest case, $A_{ij}$ is 1 if there is a connection from node $i$ to $j$, and otherwise 0. For an undirected graph, keep in mind that $A$ is a symmetric matrix ($A_{ij}=A_{ji}$). For the example graph above, we have the following adjacency matrix:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "    0 & 1 & 0 & 0\\\\\n",
    "    1 & 0 & 1 & 1\\\\\n",
    "    0 & 1 & 0 & 1\\\\\n",
    "    0 & 1 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0465b86",
   "metadata": {},
   "source": [
    "### Molecular graph\n",
    "A [molecular graph](https://en.wikipedia.org/wiki/Molecular_graph) is a labeled graph whose nodes correspond to the atoms of the compound and edges correspond to chemical bonds. It also has node features (**atom features**), edge features (**bond features**) and graph labels (chemical properties of a molecule). Next, we demonstrate a simple example of building a molecular graph (undirected). In this example, we do not consider hydrogen atoms as nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bd3c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import MolFromSmiles\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "IPythonConsole.ipython_useSVG = True  # < use SVGs instead of PNGs\n",
    "IPythonConsole.drawOptions.addAtomIndices = True  # adding indices for atoms\n",
    "IPythonConsole.drawOptions.addBondIndices = False  # not adding indices for bonds\n",
    "IPythonConsole.molSize = 200, 200\n",
    "\n",
    "# N,N-dimethylformamide (DMF)\n",
    "dmf_smiles = 'CN(C)C=O'\n",
    "mol = MolFromSmiles(dmf_smiles)\n",
    "# show molecular graph of DMF, atom indices = node indices\n",
    "mol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcba6a2",
   "metadata": {},
   "source": [
    "### Atom features\n",
    "\n",
    "| feature | description |\n",
    "| ---- | ----  |\n",
    "| atom type  | atomic number |\n",
    "| degree  | number of directly-bonded neighbor atoms, including H atoms |\n",
    "| formal charge | integer electronic charge assigned to atom |\n",
    "| hybridization | sp, sp2, sp3, sp3d, or sp3d2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cab5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATOM_FEATURES = {\n",
    "    'atom_type' : [1, 6, 7, 8, 9],  # elements: H, C, N, O, F\n",
    "    'degree' : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 'misc'],\n",
    "    'formal_charge' : [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 'misc'],\n",
    "    'hybridization' : [\n",
    "        'SP', 'SP2', 'SP3', 'SP3D', 'SP3D2', 'misc'\n",
    "        ],\n",
    "}\n",
    "\n",
    "def get_atom_fv(atom):\n",
    "    \"\"\"\n",
    "    Converts rdkit atom object to feature list of indices\n",
    "    :param atom: rdkit atom object\n",
    "    :return: list\n",
    "    \"\"\"\n",
    "    atom_fv = [\n",
    "        ATOM_FEATURES['atom_type'].index(atom.GetAtomicNum()),\n",
    "        ATOM_FEATURES['degree'].index(atom.GetTotalDegree()),\n",
    "        ATOM_FEATURES['formal_charge'].index(atom.GetFormalCharge()),\n",
    "        ATOM_FEATURES['hybridization'].index(str(atom.GetHybridization())),\n",
    "    ]\n",
    "    return atom_fv\n",
    "\n",
    "atom_fvs = [get_atom_fv(atom) for atom in mol.GetAtoms()]\n",
    "atom_fvs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c131ed",
   "metadata": {},
   "source": [
    "One chemical issue we need to pay attention to is that the N atom in DMF are `sp2` hybridized instead of `sp3`.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"img/DMF_N_sp2.png\" width=\"600\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a4906b",
   "metadata": {},
   "source": [
    "### Bond features\n",
    "\n",
    "| feature | description |\n",
    "| ---- | ----  |\n",
    "| bond type  | single, double, triple, or aromatic |\n",
    "| stereo | none, any, E/Z or cis/trans |\n",
    "| conjugated  | whether the bond is conjugated |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03427568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show indices of bonds\n",
    "IPythonConsole.drawOptions.addAtomIndices = False  # not adding indices for atoms\n",
    "IPythonConsole.drawOptions.addBondIndices = True  # adding indices for bonds\n",
    "mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7f2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOND_FEATURES = {\n",
    "    'bond_type' : [\n",
    "        'SINGLE',\n",
    "        'DOUBLE',\n",
    "        'TRIPLE',\n",
    "        'AROMATIC',\n",
    "        'misc'\n",
    "    ],\n",
    "    'stereo': [\n",
    "        'STEREONONE',\n",
    "        'STEREOZ',\n",
    "        'STEREOE',\n",
    "        'STEREOCIS',\n",
    "        'STEREOTRANS',\n",
    "        'STEREOANY',\n",
    "    ], \n",
    "    'conjugated': [False, True],\n",
    "}\n",
    "\n",
    "def get_bond_fv(bond):\n",
    "    \"\"\"\n",
    "    Converts rdkit bond object to feature list of indices\n",
    "    :param bond: rdkit bond object\n",
    "    :return: list\n",
    "    \"\"\"\n",
    "    bond_fv = [\n",
    "        BOND_FEATURES['bond_type'].index(str(bond.GetBondType())),\n",
    "        BOND_FEATURES['stereo'].index(str(bond.GetStereo())),\n",
    "        BOND_FEATURES['conjugated'].index(bond.GetIsConjugated()),\n",
    "    ]\n",
    "    return bond_fv\n",
    "\n",
    "bond_fvs = [get_bond_fv(bond) for bond in mol.GetBonds()]\n",
    "bond_fvs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42b4193",
   "metadata": {},
   "source": [
    "### Edge index\n",
    "In many cases, a list of paired node indices are used to describe edges rather than adjacency matrix. Here we use paired node indices (`edge_index`) with shape (2, num_edges) to define the edges in a graph.\n",
    "\n",
    "$$\n",
    "\\mathbf{E} = \\begin{bmatrix}\n",
    "    ..., & i, & ..., & j, & ... \\\\\n",
    "    ..., & j, & ..., & i, & ...\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Like, there has an edge between node $i$ and node $j$ (undirected graph).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4291b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index0, edge_index1 = [], []\n",
    "\n",
    "for bond in mol.GetBonds():\n",
    "    i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "    edge_index0 += [i, j]\n",
    "    edge_index1 += [j, i]\n",
    "\n",
    "edge_index = [edge_index0, edge_index1]\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbf2dbb",
   "metadata": {},
   "source": [
    "### Molecular graph data\n",
    "\n",
    "We set the density of DMF(0.944 $g/cm^3$) as the graph feature (label). Here we use [Data](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.Data.html#torch_geometric.data.Data) class in `PyG` to create a graph data for DMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac3538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# convert our data to tensors, which are used for model training\n",
    "x = torch.tensor(atom_fvs, dtype=torch.float)\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "edge_attr = torch.tensor(bond_fvs, dtype=torch.float)\n",
    "y = torch.tensor([0.944], dtype=torch.float)\n",
    "\n",
    "dmf_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "dmf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b4e523",
   "metadata": {},
   "source": [
    "## 2.2 Graph Neural Network\n",
    "\n",
    "A [graph neural network (GNN)](https://en.wikipedia.org/wiki/Graph_neural_network) is a class of artificial neural networks for processing data that can be represented as graphs. GNNs rely on [message passing methods](https://arxiv.org/abs/1704.01212), which means that nodes exchange information with the neighbors, and send \"messages\" to each other. Generally, GNNs operate in two phases: a **message passing** phase, which transmits information across the molecule to build a neural representation of the molecule, and a **readout** phase, which uses the final representation of the molecule to make predictions about the properties of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0dddb7",
   "metadata": {},
   "source": [
    "### Message passing\n",
    "\n",
    "Before looking at the math, we can try to visually understand how message passing works. The first step is that each node creates a `feature vector` that represents the `message` it wants to send to all its neighbors. In the second step, the messages are sent to the neighbors, so that a node receives one message per adjacent node. As shown in the figure below, after a message passing step, `node 1` can get the message from `node 2`, and `node 2` can get messages from `node 1`, `node 3` and `node 4`. The third step is that each node will aggregate all messages from neighbors and get a `message vector`. Then, the fourth step is that each node updates its `feature vector` based on its `message vector` and previous `feature vector`.\n",
    "\n",
    "<center width=\"100%\" style=\"padding:10px\"><img src=\"img/graph_message_passing.svg\" width=\"700px\"></center>\n",
    "\n",
    "Moreover, with the iteration of message passing, each node can obtain the feature vectors of more distant nodes and not limited to neighbors. As shown in the figure below, node `A` can get informations from node `E` and node `F` in the interation 2, which are not the neighbors of node `A`.  Node `C`, the neighbor of node `A`, can obtain the information of nodes `E` and `F` in the iteration 1, so node `A` can obtain the information of nodes `E` and `F` in the iteration 2.\n",
    "\n",
    "<center width=\"100%\" style=\"padding:10px\"><img src=\"img/messages.svg\" width=\"700px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d03889d",
   "metadata": {},
   "source": [
    "**You can also read the mathematical formulas to better understand the process of message passing.**\n",
    "\n",
    "1) Initialization\n",
    "\n",
    "Get initial hidden feature vector $h_i^0$ of node $i$ from its original node features $x_i$\n",
    "$$\n",
    "h_i^0 = I(x_i), \\quad \\forall i \\in V\n",
    "\\tag{1}\n",
    "$$\n",
    "where $I$ is initialize function\n",
    "\n",
    "2) Send message\n",
    "$$\n",
    "m_{j \\rightarrow i}^{t+1} = M(h_i^t, \\: h_j^t, \\: e_{i,j})\n",
    "\\tag{2}\n",
    "$$\n",
    "where $m_{j \\rightarrow i}^{t+1}$ is the message sent from node $j$ to $i$ at the $t+1$ iteration, $M$ is message function, and $e_{i,j}$ is the feature of edge between node $i$ and $j$\n",
    "\n",
    "3) Message aggregation\n",
    "$$\n",
    "m_i^{t+1} = \\sum_{j \\in N(i)} m_{j \\rightarrow i}^{t+1}\n",
    "\\tag{3}\n",
    "$$\n",
    "where $N(i)$ presents all neighbor nodes of node $i$, and $m_i^{t+1}$ is the aggregated message of node $i$ at the $t+1$ iteration\n",
    "\n",
    "4) Node update\n",
    "$$\n",
    "h_i^{t+1} = U(h_i^t, \\: m_i^{t+1})\n",
    "\\tag{4}\n",
    "$$\n",
    "where $h_i^t$ is the hidden feature vector of node $i$ at the $t$ iteration, and $U$ is the update function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1a10fa",
   "metadata": {},
   "source": [
    "### Readout\n",
    "The readout layers will aggregate the hidden feature vectors of all nodes and get graph-level vectors (that is, the properties we want to predict).\n",
    "\n",
    "$$\n",
    "\\hat{y} = R(\\{ h_i^T \\: | \\: i \\in V\\})\n",
    "\\tag{5}\n",
    "$$\n",
    "where $h_i^T$ is the final hidden feature vector of node $i$, $\\: \\: \\hat{y}$ is graph-level vectors (our prediction target), and $R$ is the readout function\n",
    "\n",
    "**Note that in GNNs, the $I$, $M$, $U$ and $R$ functions need to be differentiable, such as multi-layer artificial neural networks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae14f812",
   "metadata": {},
   "source": [
    "### Code example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de73f99",
   "metadata": {},
   "source": [
    "Here, we will define a GNN model using message passing neural network (MPNN) according to paper [\"Neural Message Passing for Quantum Chemistry\"](https://arxiv.org/abs/1704.01212). We just use [NNConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.NNConv.html#torch_geometric.nn.conv.NNConv) class to create message passing layers of our models. The [torch_geometric.nn](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html) module of PyG contains many different types of layers for message passing and readout, which can help us define GNN models more conveniently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67d234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import GRU\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import NNConv, MLP, global_add_pool\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "\n",
    "\n",
    "class MPNN(pl.LightningModule):\n",
    "    def __init__(self, hidden_dim, out_dim,\n",
    "                 train_data, valid_data, test_data,\n",
    "                 std, batch_size=32, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.std = std  # std of data's target\n",
    "        self.train_data = train_data\n",
    "        self.valid_data = valid_data\n",
    "        self.test_data = test_data\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        # Initial layers\n",
    "        self.atom_emb = AtomEncoder(emb_dim=hidden_dim)\n",
    "        self.bond_emb = BondEncoder(emb_dim=hidden_dim)\n",
    "        # Message passing layers\n",
    "        nn = MLP([hidden_dim, hidden_dim*2, hidden_dim*hidden_dim])\n",
    "        self.conv = NNConv(hidden_dim, hidden_dim, nn, aggr='mean')\n",
    "        self.gru = GRU(hidden_dim, hidden_dim)\n",
    "        # Readout layers\n",
    "        self.mlp = MLP([hidden_dim, int(hidden_dim/2), out_dim])\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "\n",
    "        # Initialization\n",
    "        x = self.atom_emb(data.x)\n",
    "        h = x.unsqueeze(0)\n",
    "        edge_attr = self.bond_emb(data.edge_attr)\n",
    "        \n",
    "        # Message passing\n",
    "        for i in range(3):\n",
    "            m = F.relu(self.conv(x, data.edge_index, edge_attr))  # send message and aggregation\n",
    "            x, h = self.gru(m.unsqueeze(0), h)  # node update\n",
    "            x = x.squeeze(0)\n",
    "\n",
    "        # Readout\n",
    "        x = global_add_pool(x, data.batch)\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x.view(-1)\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Here we define the train loop.\n",
    "        out = self.forward(batch, mode=\"train\")\n",
    "        loss = F.mse_loss(out, batch.y)\n",
    "        self.log(\"Train loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Define validation step. At the end of every epoch, this will be executed\n",
    "        out = self.forward(batch, mode=\"valid\")\n",
    "        loss = F.mse_loss(out * self.std, batch.y * self.std)  # report MSE\n",
    "        self.log(\"Valid MSE\", loss)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # What to do in test\n",
    "        out = self.forward(batch, mode=\"test\")\n",
    "        loss = F.mse_loss(out * self.std, batch.y * self.std)  # report MSE\n",
    "        self.log(\"Test MSE\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Here we configure the optimization algorithm.\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.lr\n",
    "        )\n",
    "        return optimizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valid_data, batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, batch_size=self.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04593bf6",
   "metadata": {},
   "source": [
    "Here, we can use [InMemoryDataset]() class in PyG to create the graph dataset of ESOL conveniently. You can also browse its [tutorial](https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_dataset.html) and [pre-defined dataset](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html) to learn about how to create graph datasets quickly by PyG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7eeec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import (\n",
    "    Data,\n",
    "    InMemoryDataset,\n",
    "    download_url,\n",
    ")\n",
    "from ogb.utils import smiles2graph\n",
    "\n",
    "\n",
    "class ESOLGraphData(InMemoryDataset):\n",
    "    \"\"\"The ESOL graph dataset using PyG\n",
    "    \"\"\"\n",
    "    # ESOL dataset download link\n",
    "    raw_url = 'https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/delaney-processed.csv'\n",
    "\n",
    "    def __init__(self, root, transform=None):\n",
    "        super().__init__(root, transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['delaney-processed.csv']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        print('Downloading ESOL dataset...')\n",
    "        file_path = download_url(self.raw_url, self.raw_dir)\n",
    "\n",
    "    def process(self):\n",
    "        # load raw data from a csv file\n",
    "        df = pd.read_csv(self.raw_paths[0])\n",
    "        smiles = df['smiles'].values.tolist()\n",
    "        target = df['measured log solubility in mols per litre'].values.tolist()\n",
    "\n",
    "        # Convert SMILES into graph data\n",
    "        print('Converting SMILES strings into graphs...')\n",
    "        data_list = []\n",
    "        for i, smi in enumerate(tqdm(smiles)):\n",
    "\n",
    "            # get graph data from SMILES\n",
    "            graph = smiles2graph(smi)\n",
    "\n",
    "            # convert to tensor and pyg data\n",
    "            x = torch.tensor(graph['node_feat'], dtype=torch.long)\n",
    "            edge_index = torch.tensor(graph['edge_index'], dtype=torch.long)\n",
    "            edge_attr = torch.tensor(graph['edge_feat'], dtype=torch.long)\n",
    "            y = torch.tensor([target[i]], dtype=torch.float)\n",
    "            data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "            data_list.append(data)\n",
    "\n",
    "        # save data\n",
    "        torch.save(self.collate(data_list), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f352ca3",
   "metadata": {},
   "source": [
    "Create, normalize and split ESOL graph dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb4e4f8-86b4-4edd-998d-897d5eb02d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.splits import RandomSplitter\n",
    "\n",
    "# create dataset\n",
    "dataset = ESOLGraphData('./esol_pyg').shuffle()\n",
    "\n",
    "# Normalize target to mean = 0 and std = 1.\n",
    "mean = dataset.data.y.mean()\n",
    "std = dataset.data.y.std()\n",
    "dataset.data.y = (dataset.data.y - mean) / std\n",
    "mean, std = mean.item(), std.item()\n",
    "\n",
    "# split data\n",
    "splitter = RandomSplitter()\n",
    "train_idx, valid_idx, test_idx = splitter.split(dataset, frac_train=0.7, frac_valid=0.1, frac_test=0.2)\n",
    "train_dataset = dataset[train_idx]\n",
    "valid_dataset = dataset[valid_idx]\n",
    "test_dataset = dataset[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a83e75-11ec-435c-815a-ebb1a9ccdb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will ask you to login to your wandb account\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"gnn-solubility\",\n",
    "           config={\n",
    "               \"batch_size\": 32,\n",
    "               \"learning_rate\": 0.001,\n",
    "               \"hidden_size\": 64,\n",
    "               \"max_epochs\": 60\n",
    "           })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee98743",
   "metadata": {},
   "source": [
    "Train and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50f7cb0-a057-4521-a047-66b4d51fd27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create an instance of our GNN.\n",
    "# Play around with the hyperparameters!\n",
    "gnn_model = MPNN(\n",
    "    hidden_dim=wandb.config[\"hidden_size\"],\n",
    "    out_dim=1,\n",
    "    std=std,\n",
    "    train_data=train_dataset,\n",
    "    valid_data=valid_dataset,\n",
    "    test_data=test_dataset,\n",
    "    lr=wandb.config[\"learning_rate\"],\n",
    "    batch_size=wandb.config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "# Define trainer: How we want to train the model\n",
    "wandb_logger = WandbLogger()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs = wandb.config[\"max_epochs\"],\n",
    "    logger = wandb_logger\n",
    ")\n",
    "\n",
    "# Finally! Training a model :)\n",
    "trainer.fit(\n",
    "    model=gnn_model,\n",
    ")\n",
    "\n",
    "# Now run test\n",
    "results = trainer.test(ckpt_path=\"best\")\n",
    "wandb.finish()\n",
    "\n",
    "# Test RMSE\n",
    "test_mse = results[0][\"Test MSE\"]\n",
    "test_rmse = test_mse ** 0.5\n",
    "print(f\"\\nMPNN model performance: RMSE on test set = {test_rmse:.4f}.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
