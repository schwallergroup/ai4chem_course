{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b6d7b6-698e-4ea2-b8e9-bad6f279fcfe",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/schwallergroup/ai4chem_course/blob/main/notebooks/02%20-%20Supervised%20Learning/training_and_evaluating_ml_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ee09d-524b-4f90-ac5c-cc37187ba4c4",
   "metadata": {},
   "source": [
    "# Week 2 tutorial - AI 4 Chemistry\n",
    "\n",
    "## Table of content\n",
    "\n",
    "0. Relevant packages\n",
    "1. Supervised learning\n",
    "2. Regression\n",
    "3. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15d3c35",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. Relevant packages\n",
    "\n",
    "### Scikit-learn\n",
    "\n",
    "Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities. We will learn to use scikit-learn to do machine learning work. You can also browse the scikit-learn [user guide](https://scikit-learn.org/stable/user_guide.html) and [tutorials](https://scikit-learn.org/stable/tutorial/index.html) for additional details.\n",
    "\n",
    "### Essential Libraries and Tools \n",
    "\n",
    "Scikit-learn depends on two other Python packages, NumPy and SciPy. For plotting and interactive development, you should also install matplotlib, IPython, and the Jupyter Notebook.\n",
    "\n",
    "- **NumPy** is one of the fundamental packages for scientific computing in Python. In scikit-learn, the NumPy array is the fundamental data structure. Any data you’re using will have to be converted to a NumPy array.\n",
    "- **SciPy** is a collection of functions for scientific computing in Python. Scikit-learn draws from SciPy’s collection of functions for implementing its algorithms.\n",
    "- **Matplotlib** is the primary scientific plotting library in Python. It provides functions for making publication-quality visualizations such as line charts, histograms, scatter plots, and so on.\n",
    "- **Pandas** Python library for data wrangling and analysis. It can ingest from a great variety of file formats and databases, like SQL, Excel files, and comma-separated values (CSV) files.\n",
    "\n",
    "### XGBoost\n",
    "\n",
    "XGBoost (eXtreme Gradient Boosting) is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. You can also browse the [XGBoost Documentation](https://xgboost.readthedocs.io/en/stable/) for additional details.\n",
    "\n",
    "### DeepChem\n",
    "\n",
    "DeepChem is a high quality open-source toolchain that democratizes the use of deep-learning in chemistry, biology and materials science. It also provides various tools for dataset loader, splitters, molecular featurization, model construction and hyperparameter tuning. You can also browse the [DeepChem Ducumentation](https://deepchem.readthedocs.io/en/latest/) for additional details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fbf541",
   "metadata": {},
   "source": [
    "We will first install the required libraries. We also need `RDKit` library to process and analyze molecules, like calculating molecular descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee96d46-2b8a-4043-8289-a5944f5311b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all libraries\n",
    "!pip install numpy scipy matplotlib scikit-learn pandas rdkit xgboost deepchem mordred pycm\n",
    "\n",
    "# Download all data\n",
    "!mkdir data\n",
    "!wget https://raw.githubusercontent.com/schwallergroup/ai4chem_course/main/notebooks/02%20-%20Supervised%20Learning/data/esol.csv -O data/esol.csv\n",
    "!wget https://raw.githubusercontent.com/schwallergroup/ai4chem_course/main/notebooks/02%20-%20Supervised%20Learning/data/toxcast_data.csv -O data/toxcast_data.csv\n",
    "!wget https://drive.switch.ch/index.php/s/3WJTVB7xHG8ZOhD/download -O data/features_tox.npy\n",
    "!wget https://drive.switch.ch/index.php/s/lFN8myikJekptjk/download -O data/y_tox.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dba2fc2",
   "metadata": {},
   "source": [
    "# 1. Supervised learning\n",
    "\n",
    "Two major types of supervised machine learning problems:\n",
    "\n",
    "- **Classification**, where the task task is to predict a class label (e.g. what color, what smell, state of aggregation, etc).\n",
    "- **Regression**, where the task is to predict a real number (e.g. solubility in water, yield, selectivity, etc).\n",
    "\n",
    "## 1.1 Algorithms\n",
    "\n",
    "- k-Nearest Neighbors (k-NN)\n",
    "- Linear Models\n",
    "- Support Vector Machines (SVM)\n",
    "- Decision Trees\n",
    "- Ensembles of Decision Trees\n",
    "  - Random forests\n",
    "  - Gradient boosting machines\n",
    "\n",
    "The package [scikit-learn](https://scikit-learn.org/stable/index.html) lets us create a large number ML models in a very conveinent way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86793e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fd6e5e-2960-4446-8a57-920f0e324129",
   "metadata": {},
   "source": [
    "### Exercise: Similarly to the linear regression model above, create a [k-NN regression model](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f1caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE\n",
    "\n",
    "# import\n",
    "' your code '\n",
    "\n",
    "# create knn model\n",
    "knn_clf = ' your code '\n",
    "\n",
    "### END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3441adba",
   "metadata": {},
   "source": [
    "Next time, you can browse the scikit-learn [user guide](https://scikit-learn.org/stable/user_guide.html) to learn about supported algorithms and how to create the model you want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bd577f-19c7-4121-b0c7-753f1e500356",
   "metadata": {},
   "source": [
    "## 1.2 Model evaluation and data splitting\n",
    "\n",
    "### Why do we need to split the dataset?\n",
    "\n",
    "We want models to learn from data so we can use them in the future, but we also need to know how good our models perform when they see new examples, and so we reserve some part of our dataset for `testing`: we keep it to evaluate how the model would do the real world, with data it hasn't seen before.\n",
    "\n",
    "In addition, typically we implement multiple models and select the best one, but how to assess which one is the best, without revealing the `test set`? Well, take another subset of the data, and this one we call `validation` set. \n",
    "\n",
    "In the end, we end up splitting our data into `training` (used for training the models), `validation` (for selecting the models), and `test` (for testing the resulting models). If you have more time, you can read [this article](https://towardsdatascience.com/how-to-split-data-into-three-sets-train-validation-and-test-and-why-e50d22d3e54c) for more details.\n",
    "\n",
    "### Evaluation metrics\n",
    "The metrics used to evaluate the ML models are very important. The choice of metrics to use influences how model performance is measured and compared. The main evaluation metrics for regression and classification tasks are illustrated below. If you have more time, you can read this [article](https://blog.knoldus.com/model-evaluation-metrics-for-machine-learning-algorithms/) for more details.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://www.oreilly.com/api/v2/epubs/9781492073048/files/assets/mlbf_0407.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a633f2c",
   "metadata": {},
   "source": [
    "## 1.3 The path to a ML model.\n",
    "0. Define the task\n",
    "1. Prepare data & split data\n",
    "2. Choose the model\n",
    "3. Train the model\n",
    "4. Evaluate the model\n",
    "5. Use the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048efaf2",
   "metadata": {},
   "source": [
    "# 2. Regression\n",
    "\n",
    "**Aqueous solubility is one of the key physical properties of interest to a medicinal or agrochemical chemist**. Solubility affects the uptake/distribution of biologically active compounds in living material and the environment, thus affecting their potential efficacy and marketability. However, solubility determination is a time-consuming experiment, and it is useful to be able to assess solubility in the absence of a physical sample. \n",
    "\n",
    "Our goal here will be to build a ML model that can predict **aqueous solubility** of organic molecules.\n",
    "\n",
    "### We will use the [ESOL dataset](https://pubs.acs.org/doi/10.1021/ci034243x) for this task.\n",
    "This dataset contains structures and water solubility data for 1128 compounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf98acc5",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf79eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load dataset from the CSV file\n",
    "esol_df = pd.read_csv('data/esol.csv')\n",
    "esol_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794e236d",
   "metadata": {},
   "source": [
    "The original dataset contains 2 columns, the `smiles` column, which encodes the molecular structure, and the `log solubility (mol/L)` columns represents aqueous solubility of molecules, which is the our target for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2950f29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get NumPy arrays from DataFrame for the input and target\n",
    "smiles = esol_df['smiles'].values\n",
    "y = esol_df['log solubility (mol/L)'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343779ed",
   "metadata": {},
   "source": [
    "### Molecular descriptor calculation\n",
    "We need to convert the SMILES strings of molecules into numerical values that can be used as input to the ML models. Many molecular descriptors can be calculated from SMILES strings using software like `RDKit`, `DeepChem` and [Mordred](https://github.com/mordred-descriptor/mordred).\\\n",
    "For this task we will use [DeepChem featurizers](https://deepchem.readthedocs.io/en/latest/api_reference/featurizers.html) to compute molecular descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5961b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we use molecular descriptors from RDKit, like molecular weight, number of valence electrons, maximum and minimum partial charge, etc.\n",
    "from deepchem.feat import RDKitDescriptors\n",
    "featurizer = RDKitDescriptors()\n",
    "features = featurizer.featurize(smiles)\n",
    "print(f\"Number of generated molecular descriptors: {features.shape[1]}\")\n",
    "\n",
    "# Drop the features containing invalid values\n",
    "import numpy as np\n",
    "features = features[:, ~np.isnan(features).any(axis=0)]\n",
    "print(f\"Number of molecular descriptors without invalid values: {features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9c63ba",
   "metadata": {},
   "source": [
    "**Exercise**: You can try to use [MACCSKeysFingerprint](https://deepchem.readthedocs.io/en/latest/api_reference/featurizers.html#maccskeysfingerprint) in [DeepChem featurizers](https://deepchem.readthedocs.io/en/latest/api_reference/featurizers.html) to compute molecular descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f941ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE\n",
    "\n",
    "# import the featurizer\n",
    "' your code '\n",
    "\n",
    "# create featurizer\n",
    "mf_featurizer = ' your code '\n",
    "\n",
    "# compute molecular descriptors\n",
    "mf_features = ' your code '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97180e44",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "Feature selection is a crucial step in machine learning that involves selecting the most relevant features (or variables) from a dataset. This process helps to improve the accuracy and efficiency of a model by reducing the amount of noise and redundancy in the data. Essentially, feature selection allows us to focus on the most important information in the dataset, while ignoring the irrelevant or redundant information.\n",
    "\n",
    "Many classes and functions in the [sklearn.feature_selection](https://scikit-learn.org/stable/modules/feature_selection.html) module can be used for feature selection.\\\n",
    "Here we show a simple feature selection using `VarianceThreshold` in `scikt-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326fe67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we removed all zero-variance features, i.e. features that have the same value in all samples.\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selector = VarianceThreshold(threshold=0.0)\n",
    "features = selector.fit_transform(features)\n",
    "print(f\"Number of molecular descriptors after removing zero-variance features: {features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138076f4",
   "metadata": {},
   "source": [
    "### Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8675b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = features\n",
    "# training data size : test data size = 0.8 : 0.2\n",
    "# fixed seed using the random_state parameter, so it always has the same split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.8, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa51e6b",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "As it turns out, different features have different scales and distributions.\\\n",
    "Think of molecular weight, which goes from **0 to around 1000 u.a.m. for small organic molecules**, and electrochemical potential, [typically between -3 and 3](https://par.nsf.gov/servlets/purl/10016877). These differences have a huge impact on ML models, which is the reason why we will normalize all the features.\n",
    "\n",
    "Many classes and functions in the [sklearn.preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html) module can be used for preprocessing data. Here we show a simple Min-Max normalization of features using `MinMaxScaler` in `scikt-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b684fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# save original X\n",
    "X_train_ori = X_train\n",
    "X_test_ori = X_test\n",
    "# transform data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1e3784",
   "metadata": {},
   "source": [
    "### Q: Is there a difference between doing data preprocessing before and after data split? Which one is better and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394d369a",
   "metadata": {},
   "source": [
    "### Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b94772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest regressor, and the default criterion is mean squared error (MSE)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "ranf_reg = RandomForestRegressor(n_estimators=10, random_state=0)  # using 10 trees and seed=0\n",
    "\n",
    "# XGBoost regressor\n",
    "from xgboost import XGBRegressor\n",
    "xgb_reg = XGBRegressor(n_estimators=10, random_state=0)  # using 10 trees and seed=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b66c0b1",
   "metadata": {},
   "source": [
    "### Train and evaluate the models\n",
    "- Mean Squared Error: $MSE$ = $\\frac{1}{n} \\Sigma_{i=1}^n({y}-\\hat{y})^2$\n",
    "- Root Mean Squared Error: $RMSE$ = $\\sqrt{MSE}$ = $\\sqrt{\\frac{1}{n} \\Sigma_{i=1}^n({y}-\\hat{y})^2}$\n",
    "\n",
    "We choose `RMSE` as the evaluation metric for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b21ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def train_test_model(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Function that trains a model, and tests it.\n",
    "    Inputs: sklearn model, train_data, test_data\n",
    "    \"\"\"\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate RMSE on training\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    model_train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    model_test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    model_train_rmse = model_train_mse ** 0.5\n",
    "    model_test_rmse = model_test_mse ** 0.5\n",
    "    print(f\"RMSE on train set: {model_train_rmse:.3f}, and test set: {model_test_rmse:.3f}.\\n\")\n",
    "\n",
    "\n",
    "# Train and test the random forest model\n",
    "print(\"Evaluating Random Forest Model.\")\n",
    "train_test_model(ranf_reg, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Train and test XGBoost model\n",
    "print(\"Evaluating XGBoost model.\")\n",
    "train_test_model(xgb_reg, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712c715f",
   "metadata": {},
   "source": [
    "### Q: Compare the results. Which model is better in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209edf19",
   "metadata": {},
   "source": [
    "**Exercise**: Try to train a [SVM model](https://scikit-learn.org/stable/modules/svm.html#regression) and evaluate it. You can create a [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR) model using default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7746533",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE\n",
    "\n",
    "# import\n",
    "' your code '\n",
    "\n",
    "# create a model\n",
    "svm_reg = ' your code '\n",
    "\n",
    "# train and evaluate the model\n",
    "' your code '\n",
    "\n",
    "### END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7de3621",
   "metadata": {},
   "source": [
    "Perfect! Now you have mastered training and evaluating the model you want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d42f3",
   "metadata": {},
   "source": [
    "## Cross-validation and hyperparamter optimization\n",
    "\n",
    "Our last topic in this section is hyperparameter optimization.\n",
    "\n",
    "As you've seen, models need some parameters as input, and we need to decide which are the best parameters (e.g. `n_estimators` in tree-based models). Many classes and functions in the [sklearn.model_selection](https://scikit-learn.org/stable/model_selection.html) module can be used for cross validation and hyperparamter optimization.\\\n",
    "Here, we use cross validation and grid search methods to optimize the paramters of random forest model. In cross-validation, the original training data is further split into training and validation sets.\n",
    "\n",
    "The class [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) does all the work for us, combining cross-validation with grid search, so we can very easily optimize the hyperparameters.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_workflow.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "<font size=5 color=\"red\">\n",
    "Note that when you call grid_search.fit(), you only pass the training data as argument.\n",
    "</font>\n",
    "Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db8e66-7724-40c4-8876-39668793ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [5, 10, 20, 30]\n",
    "}\n",
    "\n",
    "# use 5-folds cross validation during grid searching\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=0),\n",
    "    param_grid,\n",
    "    cv=5\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# re-train a model using best hyperparameters\n",
    "rf_gs = RandomForestRegressor(**grid_search.best_params_, random_state=0)\n",
    "\n",
    "print('Best paramters: ', grid_search.best_params_)\n",
    "print('Random forests performance after hyperparamter optimization:')\n",
    "train_test_model(rf_gs, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4224cf6",
   "metadata": {},
   "source": [
    "### Exercise: Compare the train and test RMSE of Random Forest models (`ranf_reg` and `rf_gs`). Which one is better? Argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bac7560-b1c1-486c-a476-be954137b132",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Classification\n",
    "\n",
    "We now turn our attention towards the other type of supervised learning: classification.\n",
    "\n",
    "Many questions in chemistry can be framed as a classification task: \n",
    "\n",
    "- Will this molecule act as a nucleophile or electrophile in my reaction?\n",
    "- What is the smell of this substance? (fruity, citrus, sweet, ...)\n",
    "\n",
    "But in this tutorial we will try to respond:\n",
    "\n",
    "<div>\n",
    "<img src=\"img/is_this_toxic.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## For this, we need data. [MoleculeNet](https://moleculenet.org/datasets-1) provides several datasets, and we'll work with `ToxCast` for prediction of toxicity.\n",
    "\n",
    "ToxCast is a dataset containing `toxicology data for a large library of compounds based on in vitro high-throughput screening, including experiments on over 600 tasks`.\n",
    "\n",
    "Let's see if one of our models can tell what molecules are toxic!\n",
    "\n",
    "This is super useful for instance in drug discovery, where we want to know if a molecule has potential as a drug, **even before we synthesize it**.\n",
    "\n",
    "The steps we follow are similar to those we saw for regression:\n",
    "\n",
    "1. Prepare & split data\n",
    "2. Choose a model\n",
    "3. Train the model\n",
    "4. Evaluate the model\n",
    "5. Use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd83e8dd-2277-42c6-b99a-2eec6b0e44f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load clintox data from the data directory and see what it contains\n",
    "df_toxicity = pd.read_csv(\"data/toxcast_data.csv\")\n",
    "df_toxicity.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d241778d-4b07-44cd-8713-c11c736f5e5c",
   "metadata": {},
   "source": [
    "This dataset contains the molecule SMILES, plus a lot of data coming from different [toxicity measurements](https://www.epa.gov/chemical-research/exploring-toxcast-data).\n",
    "\n",
    "We will stick to `TOX21_TR_LUC_GH3_Antagonist`, as is the one for which we have more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51abea66-40f1-4183-b79d-ccee003a17f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tox = df_toxicity.loc[:,[\"smiles\",\"TOX21_TR_LUC_GH3_Antagonist\"]].dropna()\n",
    "df_tox.columns = [\"smiles\", \"toxic\"]\n",
    "df_tox.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3cfd3d-5090-4a34-8fde-c7e6f157e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "# Visualize some of the molecules of this dataset\n",
    "n=6\n",
    "df_sample=df_tox.sample(n)\n",
    "\n",
    "smiles = df_sample.smiles.values\n",
    "legend = df_sample.toxic.values\n",
    "molecs = [Chem.MolFromSmiles(s) for s in smiles]\n",
    "\n",
    "Draw.MolsToGridImage(\n",
    "    molecs,\n",
    "    subImgSize=(600,300),\n",
    "    legends=[\"Toxic\" if i==1 else \"Non toxic\" for i in legend]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d934d7-2b65-436f-a02f-f7115c6c977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many toxic molecules are in the dataset?\n",
    "counts = df_tox[\"toxic\"].value_counts()\n",
    "\n",
    "print(f\"The dataset contains {counts.sum()} molecules; {counts.iloc[1]} of them are toxic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7330eb2-1955-4f1d-92a6-7a094ad50333",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Now, we will calculate some molecular descriptors using the [mordred package](http://mordred-descriptor.github.io/documentation/master/descriptors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b862e-4f02-445f-96c5-658bb7d1c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "from deepchem.feat import MordredDescriptors\n",
    "\n",
    "featurizer = MordredDescriptors(ignore_3D=True)\n",
    "features = featurizer.featurize(\"CCC\")\n",
    "print(\"Number of molecular descriptors:\", features.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711d031a-ca7c-4cf2-a959-389fd711cdc2",
   "metadata": {},
   "source": [
    "This can take a while (at least 40 minutes), you can use this time to explore a bit the more than 1600 descriptors from mordred!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf18155-356f-4305-9727-1ebf3f6d90ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_raw = df_tox.smiles.apply(lambda x: featurizer.featurize(x))\n",
    "y_raw = df_tox.toxic\n",
    "\n",
    "# As you see from the warnings, mordred couldn't calculate features for a few molecules (but don't worry!)\n",
    "# Remove these molecules from the dataset\n",
    "\n",
    "# Featurizer should return array of size (1, number_of_features)\n",
    "missing = X_raw.apply(lambda x: x.shape == (1, features.shape[1]))\n",
    "\n",
    "print(f\"Dropping {(~missing).sum()} molecules that couldn't be featurized.\")\n",
    "X = X_raw[missing].values\n",
    "y = y_raw[missing].values\n",
    "\n",
    "X = np.concatenate(X)\n",
    "\n",
    "# Save\n",
    "np.save(\"data/features_tox.npy\", X)\n",
    "np.save(\"data/y_tox.npy\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6dc3fc-87f0-4198-a637-bb602803c49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"data/features_tox.npy\")\n",
    "y = np.load(\"data/y_tox.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df19e380-eaf6-4f72-851c-ccb226f1611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Which molecules couldn't be featurized? Why?\n",
    "# Using code from above, visualize the faulty molecules.\n",
    "\n",
    "##################\n",
    "# Your code here #\n",
    "##################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4920f04b-5cb3-4057-bac5-e7ed8eca57dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data splitting.\n",
    "\n",
    "For this exercise, we will do a simple train/test split as we will not optimize hyperparameters. (Maybe bonus exercise here?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c46051-b3dc-4931-91ed-7904dcc1d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train data size : test data size = 0.8 : 0.2\n",
    "# fixed seed using the random_state parameter, so it always has the same split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    train_size=0.8,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(f\"Train set size is {X_train.shape[0]} rows, test set size is {X_test.shape[0]} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5de382e-50de-4211-911a-08ed09bfb4a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model\n",
    "\n",
    "Let's train a [Random Forest Classification](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d6b738-759b-4b47-9c0f-fa0e7ca8e9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=10,\n",
    "    random_state=0\n",
    ")\n",
    "rf_clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13029e24-4b8c-45c5-9bd9-be41479dba50",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise: You have already seen some cool classification algorithms in class.\n",
    "\n",
    "In this exercise, your task is to implement your 2 favorite algorithms using sklearn. \n",
    "\n",
    "Recommendations: \n",
    "\n",
    "- You can choose from [Logistic Regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), [Support Vector Machines](https://scikit-learn.org/stable/modules/svm.html#classification), [Gradient Boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting), or any other from the [sklearn documentation](https://scikit-learn.org/stable/supervised_learning.html).\n",
    "- Give a different name to each model. For instance, our Random Forest model is `rf_clf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567ec970-200c-4d22-bd40-e2e791faa004",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Your code here #\n",
    "##################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf09c91-bdcc-450b-ac7e-0799111fb453",
   "metadata": {},
   "source": [
    "## After training these models, let's see which one worked best! \n",
    "\n",
    "For the evaluation of classification models, **we use different metrics** than evaluation of regression models. \\\n",
    "You can read more about each metric [here](https://scikit-learn.org/stable/modules/model_evaluation.html), but for this tutorial we will use [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score), [ROC-AUC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score), and [F1 Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121851be-f4b9-4ac0-83e0-269ecff3de7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's evaluate our models \n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "\n",
    "# Exercise: Use your models to predict the toxicity of the molecules on the test set.\n",
    "\n",
    "##################\n",
    "# Your code here #\n",
    "##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67108633-40d3-4cae-b45d-6f5853254dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# Let's calculate accuracy_score for all our models\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Accuracy of Random Forest Classifier is {acc_rf:.3f}\")\n",
    "auc_rf = roc_auc_score(y_test, y_pred_rf)\n",
    "print(f\"ROC-AUC of Random Forest Classifier is {auc_rf:.3f}\")\n",
    "f1s_rf = f1_score(y_test, y_pred_rf)\n",
    "print(f\"F1 Score of Random Forest Classifier is {f1s_rf:.3f}\")\n",
    "\n",
    "# Exercise: Calculate the 3 metrics for every model you trained, and compare results\n",
    "\n",
    "##################\n",
    "# Your code here #\n",
    "##################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eae97b-af7f-4d3a-a834-48ed7a400587",
   "metadata": {},
   "source": [
    "\n",
    "Find out what each metric is telling us. Should we trust such high accuracies? Why is accuracy so high compared to the other metrics?\n",
    "\n",
    "YOUR ANSWER:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71f8e9d-d0ef-40b4-945e-fa85507cb922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO let's also do confusion matrix\n",
    "from pycm import ConfusionMatrix\n",
    "\n",
    "cm = ConfusionMatrix(actual_vector=y_test, predict_vector=y_pred_rf)\n",
    "cm.relabel(mapping={0:\"Non Toxic\", 1:\"Toxic\"})\n",
    "cm.plot(number_label=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c192a-ecbf-48f4-81fc-8d67fbefc7b4",
   "metadata": {},
   "source": [
    "# So after all what, is my molecule toxic? Let's see what our model says!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b927033-af33-4e0a-b561-5a89e839c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def is_this_toxic(molecule, model):\n",
    "    \"\"\"\n",
    "    Ask model if the input molecule (smiles) is toxic or not.\n",
    "    \"\"\"\n",
    "    \n",
    "    mol = Chem.MolFromSmiles(molecule)\n",
    "\n",
    "    # Calculate features\n",
    "    X_my_mol = featurizer.featurize(molecule)\n",
    "\n",
    "    # Get model prediction\n",
    "    is_toxic = model.predict(X_my_mol)\n",
    "    is_toxic = \"This molecule is toxic!\" if is_toxic else \"This is not toxic :)\"\n",
    "\n",
    "    img = Draw.MolsToGridImage(\n",
    "        [mol],\n",
    "        subImgSize=(600,300),\n",
    "        legends=[is_toxic],\n",
    "        molsPerRow=1\n",
    "    )\n",
    "    display(img)\n",
    "    \n",
    "    \n",
    "\n",
    "# Define molecule here\n",
    "molecule = \"O=C1N(C)C(C2=C(N=CN2C)N1C)=O\"\n",
    "is_this_toxic(molecule, model=rf_clf)\n",
    "\n",
    "\n",
    "# Exercise: Test with your own molecule!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095f403e-d4e8-4272-8e50-0807c4a9b21a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Should I trust this though?: Interpretability and explainability.\n",
    "\n",
    "> Cool, our models know stuff, but we also want to know! \\\n",
    "> What do they look at when they predict toxicity? Is there a key feature?\n",
    "\n",
    "\\\n",
    "Model explainability is a critical component of machine learning that seeks to provide insights into how a model arrives at its predictions or decisions. In other words, it aims to make the \"black box\" of machine learning models more transparent, so that we can understand the factors that are driving the model's output.\n",
    "\n",
    "There are many different methods for achieving model explainability (more on this [here](https://www.kaggle.com/learn/machine-learning-explainability)). \\\n",
    "These techniques can help us identify which features or variables are most important in driving the model's output, and can provide insights into the model's decision-making process.\n",
    "\n",
    "### Let's explore ways of measuring feature importance, which will tell us what our models are looking out when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e33561-72c1-4b3a-880c-93aa79825a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.Series(rf_clf.feature_importances_, name=\"importance\")\n",
    "importances.index += 1\n",
    "std = pd.Series(np.std([tree.feature_importances_ for tree in rf_clf.estimators_], axis=0), \n",
    "                name=\"std\")\n",
    "\n",
    "importances = pd.concat([importances, std], axis=1)\n",
    "importances = importances.sort_values(by=\"importance\", ascending=False).iloc[:20]\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "importances[\"importance\"].plot.bar(yerr=importances[\"std\"], ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ca8ce8-6a53-4f72-af0c-6399a0a13c00",
   "metadata": {},
   "source": [
    "---\n",
    "What can we learn from these results? \\\n",
    "Go to [Mordred documentation](http://mordred-descriptor.github.io/documentation/master/descriptors.html) and find these features. What are they, and do they make any sense?\n",
    "\n",
    "### Discuss\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e1b1e114f4dae097b9e32029c5d22d73dc21a5dd723446d46774bd2adced9390"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
