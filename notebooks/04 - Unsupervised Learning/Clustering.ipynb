{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/schwallergroup/ai4chem_course/blob/main/notebooks/04 - Unsupervised Learning/Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning: Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scikit-learn-extra rdkit plotly\n",
    "\n",
    "!mkdir data/\n",
    "!wget https://raw.githubusercontent.com/schwallergroup/ai4chem_course/main/notebooks/04%20-%20Unsupervised%20Learning/data/unknown_clusters.csv -O   data/unknown_clusters.csv\n",
    "!wget https://raw.githubusercontent.com/schwallergroup/ai4chem_course/main/notebooks/04%20-%20Unsupervised%20Learning/utils.py -O utils.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from rdkit import DataStructs\n",
    "from rdkit.ML.Cluster import Butina\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "from rdkit.Chem import AllChem, Descriptors, MolFromSmiles, rdMolDescriptors\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "from utils import plot_3d, plot_2d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering \n",
    "\n",
    "> Clustering is a powerful unsupervised learning technique that aims to **group similar data points** together based on their features or attributes. \n",
    "\n",
    "We can use this to **gain insights into the underlying structure** of large and complex datasets, and to **identify patterns** and relationships that may not be immediately apparent.\n",
    "\n",
    "In this notebook, we will explore four different clustering algorithms:\n",
    "\n",
    "- KMeans\n",
    "- KMedoids\n",
    "- DBSCAN\n",
    "- Butina clustering\n",
    "\n",
    "These algorithms are widely used in a variety of fields, including chemistry, and can help researchers better understand data and make informed decisions. \n",
    "\n",
    "### In this notebook we will:\n",
    "\n",
    "- [ ] Generate a synthetic dataset (a toy example).\n",
    "- [ ] Apply each algorithm to a synthetic dataset.\n",
    "- [ ] Evaluate performance using clustering metrics.\n",
    "- [ ] Apply all this to a real-world chemistry dataset.\n",
    "\n",
    "In the last exercise, you will find the ideal number of clusters, using metrics like silhouette and inertia.\n",
    "\n",
    "By the end of this notebook, you will have a better understanding of **how clustering works** and **how to apply different clustering algorithms to real-world problems**.\n",
    "\n",
    "--- "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start by generating a synthetic clustering dataset.\n",
    "\n",
    "We will use the *make_blobs* function from scikit-learn. \n",
    "\n",
    "The *make_blobs* function generates a set of random data points with a Gaussian distribution. The data points are generated in clusters, with each cluster representing a group of points that have similar features. \n",
    "\n",
    "The function returns arrays `X` and `y`, with `X` containing the coordinates of each data point, and `y` containing the labels of each data point, indicating which cluster it belongs to.\n",
    "\n",
    "By using different values for the parameters of the *make_blobs* function, we can generate synthetic datasets with different characteristics of the clusters, different numbers of clusters, features, and standard deviations. \n",
    "\n",
    "We can use these datasets to evaluate different clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic clustered data\n",
    "n_clusters = 4\n",
    "n_features = 512\n",
    "\n",
    "# Define custom cluster standard deviations\n",
    "cluster_stds = [0.5, 1.5, 1, 2.0]\n",
    "n_samples = [200, 300, 100, 150]\n",
    "cluster_centers = np.random.randint(-5, 5, size=(n_clusters, n_features))\n",
    "\n",
    "X, y = make_blobs(n_samples=n_samples, centers=None, cluster_std=cluster_stds, n_features=n_features, center_box=(-1, 1))\n",
    "\n",
    "# Convert the data to binary to emulate molecular fingerprints\n",
    "X_binary = np.where(X > 0, 1, 0)  # Binary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coordinates for plotting using PCA\n",
    "pca = PCA(n_components=3)\n",
    "coords = pca.fit_transform(X_binary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means\n",
    "\n",
    "> **k-means** is one of the most popular clustering algorithms in machine learning.\n",
    "\n",
    "To use it, you need to **select a number of clusters** `k`.\n",
    "\n",
    "- The algor\n",
    "ithm will then randomly select `k` points --centroids-- in the feature space, and assign each point in the dataset to the *closest centroid*, thereby defining `k` clusters.\n",
    "\n",
    "- Next, new centroids are calculated using the mean of each cluster, and every point is reassigned to the closest centroid.\n",
    "\n",
    "- This process continues until the centroids stop changing, or a predetermined number of iterations is reached.\n",
    "\n",
    "\n",
    "> KMeans is widely used in various applications, including image segmentation, market segmentation, and data mining. It is a simple yet powerful algorithm that can efficiently cluster large datasets. However, its performance can be highly dependent on the initialization of the centroids, and it may not work well with datasets that have non-spherical or overlapping clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining KMeans method with n_clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init=10, init='k-means++')\n",
    "kmeans.fit(X_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can **visualize the result of k-means** on our dataset by projecting the data into a lower-dimensional space, and coloring the data points based on the clusters. \n",
    "\n",
    "> The code for doing this is done for you in `utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "plot_2d(coords, y, title=\"Actual Clusters\", ax=ax1)\n",
    "plot_2d(coords, kmeans.labels_, title=\"KMeans Predicted Clusters\", ax=ax2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that **k-means was able to find the four distinct clusters**, which closely match the actual clusters generated by the make_blobs function. \n",
    "\n",
    "**Note that the specific colors (labels) used are not important, what matters is that the algorithm is able to separate the different clusters from each other.**\n",
    "\n",
    "--\n",
    "\n",
    "Sometimes, looking at the data in 2D may not be enough to fully understand the underlying structure and patterns.\n",
    "\n",
    "If that's the case, we can use more PCA components, and **plot the data in 3D**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d(coords, labels=kmeans.labels_, title=\"KMeans clustering\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-medoids\n",
    "\n",
    "K-medoids is a clustering algorithm **similar to KMeans, with some key differences**. \n",
    "\n",
    "Like k-means, k-medoids is aims to split the dataset into a predefined `k` number of clusters. However, K-medoids uses *medoids* (i.e., the most centrally located data point in each cluster) instead of *centroids* as the representative point for each cluster.\n",
    "\n",
    "- This makes K-medoids **more robust to noise and outliers**, and allows it to handle non-spherical and non-convex clusters.\n",
    "- The central point of the cluster with KMedoids has to be **a point from the dataset**, while k-means uses any point in the space.\n",
    "\n",
    "This difference is illustrated here:\n",
    "\n",
    "\n",
    "<div align=\"left\">\n",
    "<img src=\"img/K-MeansAndK-Medoids.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "K-means typically uses the Euclidean distance metric, while K-medoids work with **any distance metric**, making it more versatile and adaptable to different types of datasets.\n",
    "\n",
    "#### Try different distance metrics here, and see how the clustering changes!\n",
    "\n",
    "Available options include \n",
    "\n",
    "- euclidean\n",
    "- jaccard\n",
    "- cityblock\n",
    "- cosine\n",
    "- l2\n",
    "- minkowski\n",
    "\n",
    "\n",
    "> You can read more about distance metrics ![here](https://medium.com/geekculture/7-important-distance-metrics-every-data-scientist-should-know-11e1b0b2ebe3) and find all the available options ![here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_metric = 'euclidean'  # Change me!\n",
    "\n",
    "kmedoids = KMedoids(\n",
    "    n_clusters=n_clusters,\n",
    "\trandom_state=42,\n",
    "\tinit='k-medoids++',\n",
    "\tmetric=d_metric,\n",
    "\tmax_iter=50000\n",
    ")\n",
    "kmedoids.fit(X_binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here are the results from \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "plot_2d(coords, y, title=\"Actual Clusters\", ax=ax1)\n",
    "plot_2d(coords, kmedoids.labels_, title=f\"K-medoids Predicted Clusters\\n(Distance = {d_metric})\", ax=ax2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d(coords, labels=kmedoids.labels_, title=f\"KMedoids clustering\\n(Distance = {d_metric})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "\n",
    "DBSCAN is a clustering algorithm that groups data points based on their density. \n",
    "\n",
    "It can find **arbitrarily shaped clusters** and is **robust to noise**. Unlike K-means and K-medoids, DBSCAN **does not require specifying the number of clusters beforehand**.\n",
    "\n",
    " The key idea behind DBSCAN is that a cluster is a dense region of points separated from other dense regions by areas of lower point density.\n",
    "\n",
    "DBSCAN requires two parameters:\n",
    "\n",
    "- eps (epsilon): The maximum distance between two points to be considered as neighbors.\n",
    "- min_samples: The minimum number of points required to form a dense region (core points).\n",
    "\n",
    "The algorithm works by defining a neighborhood around each data point and grouping together points that are close to each other based on the eps parameter. If a neighborhood contains at least min_samples points, the point is considered as a core point. Points that are not core points but are reachable from a core point belong to the same cluster as the core point. Points that are not reachable from any core point are treated as noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.5, min_samples=5, metric='jaccard')\n",
    "dbscan.fit(X_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "plot_2d(coords, y, title=\"Actual Clusters\", ax=ax1)\n",
    "plot_2d(coords, dbscan.labels_, title=\"DBSCAN Predicted Clusters\", ax=ax2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Butina Clustering Algorithm\n",
    "\n",
    "The Butina clustering algorithm is designed for handling **large datasets with binary features**. It is particularly **useful in chemoinformatics for clustering molecular fingerprints**, which are binary representations of molecular structures.\n",
    "\n",
    "Butina clustering algorithm is a single-linkage hierarchical clustering method that merges clusters based on a user-defined similarity threshold (cutoff). See more ![in the paper](https://pubs.acs.org/doi/full/10.1021/ci9803381).\n",
    "\n",
    "> One of its main advantages over other clustering methods is the ability to work well with binary data and non-Euclidean distance metrics, like Jaccard.\n",
    "\n",
    "To set the cutoff parameter for Butina clustering, follow these steps:\n",
    "\n",
    "1. Calculate the pairwise distances (using Jaccard or another suitable metric) for your dataset.\n",
    "\n",
    "2. Visualize the distribution of distances by plotting a histogram.\n",
    "\n",
    "3. Analyze the histogram and choose a cutoff value that corresponds to a reasonable threshold for distinguishing between similar and dissimilar data points.\n",
    "\n",
    "Once you have chosen an appropriate cutoff value, use it as the input parameter for the Butina clustering algorithm.\n",
    "\n",
    "#### You can experiment with different cutoff values to find the one that produces the most satisfactory clustering results for your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pairwise_distances(X_binary.astype(bool), metric='jaccard')\n",
    "\n",
    "plt.hist(distances.flatten(), bins=50)\n",
    "plt.xlabel(\"Jaccard Distance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ButinaClustering:\n",
    "    def __init__(self, cutoff=0.8, metric='jaccard'):\n",
    "        self.cutoff = cutoff\n",
    "        self.metric = metric\n",
    "\n",
    "    def fit(self, x):\n",
    "        \"\"\"\n",
    "        Perform Butina clustering on a set of fingerprints.\n",
    "\n",
    "        :param x: A numpy array of binary data\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        # Calculate the distance matrix\n",
    "        distance_matrix = []\n",
    "        x = x.astype(bool)\n",
    "        for i in range(1, len(x)):\n",
    "            distances = pairwise_distances(x[i,:].reshape(1, -1), x[:i,:], metric=self.metric)\n",
    "            distance_matrix.extend(distances.flatten().tolist())\n",
    "\n",
    "        # Perform Butina clustering\n",
    "        clusters = Butina.ClusterData(distance_matrix, len(x), self.cutoff, isDistData=True)\n",
    "        self.clusters = clusters\n",
    "\n",
    "        # Assign cluster labels to each data point\n",
    "        cluster_labels = np.full(len(x), -1, dtype=int)\n",
    "        for label, cluster in enumerate(clusters):\n",
    "            for index in cluster:\n",
    "                cluster_labels[index] = label\n",
    "\n",
    "        self.labels_ = cluster_labels\n",
    "\n",
    "\n",
    "    def fit_predict(self, x):\n",
    "      self.fit(x)\n",
    "      return self.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.65  # Change me!\n",
    "\n",
    "butina = ButinaClustering(cutoff=cutoff, metric='jaccard')\n",
    "butina.fit(X_binary)\n",
    "\n",
    "print(f\"{len(butina.clusters)} clusters were found with a cutoff = {cutoff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "plot_2d(coords, y, title=\"Actual Clusters\", ax=ax1)\n",
    "plot_2d(coords, butina.labels_, title=f\"Butina Predicted Clusters\\n(cutoff = {cutoff})\", ax=ax2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "As should be clear from the above exercises, all the algorithms give **different results**, and the results also **depend on the parameters chosen**. The questino is then,\n",
    "\n",
    "> How do I know if the results from the algorithm are good?\n",
    "\n",
    "\n",
    "In the next section, we will evaluate how clustering metrics like **inertia** and **silhouette** score can help us uncover the optimal number of clusters when using K-means or K-medoids algorithm.\n",
    "\n",
    "\n",
    "### Inertia\n",
    "Inertia is the sum of squared distances between each data point and its assigned cluster centroid. \n",
    "\n",
    "> Inertia is a measure of how tightly grouped the data points are within each cluster.\n",
    "\n",
    "A **lower inertia value** indicates that the data points within a cluster are closer to their centroid, which is **desirable**. \n",
    "\n",
    "But beware, inertia can be **sensitive to the number of clusters**, as increasing the number of clusters will generally reduce the inertia. \n",
    "**Therefore, selecting the optimal number of clusters based on inertia alone can lead to overfitting**.\n",
    "\n",
    "\n",
    "### Silhouette Score\n",
    "\n",
    "> The silhouette score is a measure of how similar a data point is to its own cluster compared to other clusters. \n",
    "\n",
    "The silhouette score ranges from -1 to 1. A **high silhouette score** indicates that **data points are well-matched to their own cluster** and poorly matched to neighboring clusters. A negative silhouette score implies that data points might have been assigned to the wrong cluster. \n",
    "\n",
    "The silhouette score can be **more robust than inertia** in determining the optimal number of clusters, as it considers both cohesion (how closely related the data points within a cluster are) and separation (how distinct the clusters are from each other).\n",
    "\n",
    "---\n",
    "\n",
    "To compare inertia and silhouette scores and determine the optimal number of clusters for the data, you can follow these steps:\n",
    "\n",
    "1. Loop through a range of cluster numbers (e.g., 2 to 10) and fit the clustering algorithms (KMeans, KMedoids, etc.) for each number of clusters.\n",
    "2. Calculate and store the inertia and silhouette scores for each clustering model and each number of clusters.\n",
    "3. Plot the inertia and silhouette scores as a function of the number of clusters.\n",
    "4. Examine the plots to determine the optimal number of clusters. \n",
    "\n",
    "> For the inertia plot, look for an \"elbow\" point, where the rate of decrease in inertia starts to level off. \n",
    "> For the silhouette plot, look for the highest silhouette score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 4\n",
    "algorithms = {\n",
    "    'KMeans': KMeans(n_clusters=n_clusters, n_init=10, init='k-means++', random_state=42),\n",
    "    'KMedoids': KMedoids(n_clusters=n_clusters, init='k-medoids++', metric='jaccard', random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inertia_and_silhouette(data, algorithms, min_clusters, max_clusters):\n",
    "    \n",
    "    for name, algorithm in algorithms.items():\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        fig.suptitle(f'{name}')\n",
    "\n",
    "        inertia = []\n",
    "        silhouette_scores = []\n",
    "        for n_clusters in range(min_clusters, max_clusters + 1):\n",
    "            algorithm.set_params(n_clusters=n_clusters)\n",
    "            labels = algorithm.fit_predict(data)\n",
    "            inertia.append(algorithm.inertia_)\n",
    "            silhouette_scores.append(silhouette_score(data, labels))\n",
    "        \n",
    "        ax1.plot(range(min_clusters, max_clusters + 1), inertia, label=name)\n",
    "        ax2.plot(range(min_clusters, max_clusters + 1), silhouette_scores, label=name)\n",
    "    \n",
    "    ax1.set_xlabel(\"Number of clusters\")\n",
    "    ax1.set_ylabel(\"Inertia\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.set_xlabel(\"Number of clusters\")\n",
    "    ax2.set_ylabel(\"Silhouette Score\")\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_clusters = 2\n",
    "max_clusters = 10\n",
    "\n",
    "plot_inertia_and_silhouette(X_binary.astype(bool), algorithms, min_clusters, max_clusters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise\n",
    "\n",
    "Now it's your turn to find the optimal number of clusters in a chemical dataset using K-means or K-medoids algorithms and the inertia vs. silhouette metrics. \n",
    "\n",
    "Using the `plot_inertia_and_silhouette` function estimate the correct number of clusters in the `unknown_clusters` dataset.\n",
    "\n",
    "- Read the dataset in data/unknown_clusters.csv. NOTE: Use the variable `data_ex`\n",
    "- Featurize SMILES using Morgan fingerprints. NOTE: Use the variable `X_ex` to store the featurized SMILES.\n",
    "- Run the `plot_inertia_and_silhouette` function to estimate the optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "data_ex =\n",
    "# ...\n",
    "X_ex =\n",
    "# ... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've estimated the number of clusters, update the N_CLUSTERS variable with the correct number and run a visualization function to see some of the molecules in your clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "N_CLUSTERS = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "# Perform clustering using KMeans and KMedoids\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS, n_init=10, random_state=42).fit(X_ex)\n",
    "kmedoids = KMedoids(n_clusters=N_CLUSTERS, init='k-medoids++', metric='jaccard', random_state=42).fit(X_ex.astype(bool))\n",
    "\n",
    "\n",
    "# Function to select a few representative molecules from each cluster\n",
    "def plot_representative_molecules(labels, smiles, n_clusters, n_molecules=5):\n",
    "    for i in range(n_clusters):\n",
    "        cluster_indices = np.where(labels == i)[0]\n",
    "        molecules = [Chem.MolFromSmiles(smile) for smile in smiles]\n",
    "        cluster_molecules = [molecules[idx] for idx in cluster_indices]\n",
    "\n",
    "        # Select the first n_molecules from the cluster\n",
    "        selected_molecules = cluster_molecules[:n_molecules]\n",
    "\n",
    "        # Plot the selected molecules\n",
    "        img = Draw.MolsToGridImage(selected_molecules, molsPerRow=n_molecules, subImgSize=(200, 200))\n",
    "        print(f\"Cluster {i+1}:\")\n",
    "        display(img)\n",
    "\n",
    "# Plot the representative molecules for KMeans\n",
    "print(\"KMeans Clusters:\")\n",
    "plot_representative_molecules(kmeans.labels_, data_ex['smiles'], N_CLUSTERS)\n",
    "\n",
    "# Plot the representative molecules for KMedoids\n",
    "print(\"KMedoids Clusters:\")\n",
    "plot_representative_molecules(kmedoids.labels_, data_ex['smiles'], N_CLUSTERS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "additive_bo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c8fd31767018617c963c6386c67ff85ad8a2ec47e81bbb467830b2728b4f39b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
